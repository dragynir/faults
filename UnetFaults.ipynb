{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UnetFaults.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFWVmG0KmepR",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIBIhmRAmwas",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "cellView": "form",
        "outputId": "9defe86f-9688-4589-9926-555ed2044b75"
      },
      "source": [
        "#@title imports\n",
        "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D, GlobalAveragePooling2D, GlobalMaxPooling2D, AveragePooling2D, ZeroPadding2D\n",
        "from keras.layers import Input, Dense,Concatenate, Flatten, Reshape, Permute, BatchNormalization, Dropout, Activation, InputLayer, Conv2DTranspose\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.models import model_from_json, Model, Sequential\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.advanced_activations import PReLU\n",
        "from keras.layers.advanced_activations import ELU\n",
        "from keras.layers import Lambda\n",
        "from keras.losses import binary_crossentropy, sparse_categorical_crossentropy\n",
        "from keras.layers.merge import concatenate\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import keras"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nEm1H3b67twf",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title compare true faults images with predicted\n",
        "import pylab as plt\n",
        "\n",
        "def _check_isnan(\n",
        "    image, \n",
        "    map_true,\n",
        "    \n",
        "    map_pred,\n",
        "    titles,\n",
        "):\n",
        "    if np.all(np.isnan(image)):\n",
        "        image = np.zeros(image.shape)\n",
        "        \n",
        "    if np.all(np.isnan(map_true)):\n",
        "        map_true = np.zeros(image.shape)\n",
        "        \n",
        "    if np.all(np.isnan(map_pred)):\n",
        "        map_pred = np.zeros(image.shape)\n",
        "        \n",
        "    titles = np.squeeze(np.array(titles))\n",
        "    \n",
        "    if not ((len(titles.shape)==1) & (len(titles) >= image.shape[0])):\n",
        "        \n",
        "        titles = np.arange(image.shape[0])\n",
        "    \n",
        "    return image, map_true, map_pred, titles\n",
        "\n",
        "def _check_shape(\n",
        "    image, \n",
        "    map_true,\n",
        "    map_pred,\n",
        "):\n",
        "    if not image.shape == map_true.shape:\n",
        "        raise Exception(\n",
        "            'image.shape must be equal map_true.shape.'\n",
        "            'The shape of image was: {}.'\n",
        "            'The shape of map_true was: {}'.format(image.shape, map_true.shape)\n",
        "        )\n",
        "        \n",
        "    if not image.shape == map_pred.shape:\n",
        "        raise Exception(\n",
        "            'image.shape must be equal map_pred.shape. '\n",
        "            'The shape of image was: {}.'\n",
        "            ' The shape of map_pred was: {}'.format(image.shape, map_pred.shape)\n",
        "        )\n",
        "\n",
        "\n",
        "def _check_input(\n",
        "    image, \n",
        "    map_true,\n",
        "    map_pred,\n",
        "    titles,\n",
        "):\n",
        "    if len(image.shape) < 3:\n",
        "        image = image[None, ...].copy()\n",
        "    \n",
        "    image, map_true, map_pred, titles = _check_isnan(image, map_true, map_pred, titles)\n",
        "    \n",
        "    if len(map_true.shape) < 3:\n",
        "        map_true = map_true[None, ...].copy()\n",
        "    \n",
        "    if len(map_pred.shape) < 3:\n",
        "        map_pred = map_pred[None, ...].copy()\n",
        "        \n",
        "    _check_shape(image, map_true, map_pred)\n",
        "    \n",
        "    return image, map_true, map_pred, titles\n",
        "\n",
        "\n",
        "def show_image_dashboard(\n",
        "    image, \n",
        "    map_true=np.nan, \n",
        "    map_pred=np.nan, \n",
        "    threshold=.5, \n",
        "    alpha=.5, \n",
        "    titles=np.nan, \n",
        "    scale_map=True,\n",
        "    fig_hight=3,\n",
        "    n_cols=3,\n",
        "    image_cmap='seismic',\n",
        "    map_true_cmap='Greys_r',\n",
        "    map_pred_cmap='Greens',\n",
        "    axis_off=True,\n",
        "):\n",
        "    \n",
        "    title_format = '{}'\n",
        "    image, map_true, map_pred, titles = _check_input(image, map_true, map_pred, titles)\n",
        "    \n",
        "    n_rows = image.shape[0] / n_cols\n",
        "    n_rows = np.int32(np.floor(n_rows)) + 1\n",
        "    \n",
        "    \n",
        "    fig_width = fig_hight * int(image.shape[2] / image.shape[1]) * n_cols\n",
        "    \n",
        "    j_zx = 0\n",
        "    for jr in range(n_rows):\n",
        "        _image = image[jr*n_cols: (jr+1)*n_cols].copy()\n",
        "        _map_true = map_true[jr*n_cols: (jr+1)*n_cols].copy()\n",
        "        _map_pred = map_pred[jr*n_cols: (jr+1)*n_cols].copy()\n",
        "        \n",
        "        _, axs = plt.subplots(figsize=(fig_width, fig_hight), ncols=n_cols, nrows=1)\n",
        "        for i in range(n_cols):\n",
        "            j_img = jr*n_cols + i\n",
        "            if j_img >= image.shape[0]:\n",
        "                continue\n",
        "            cur_image = image[j_img].copy()\n",
        "            cur_map_true = map_true[j_img].copy()\n",
        "            cur_map_pred = map_pred[j_img].copy()\n",
        "            \n",
        "            cur_map_true = np.float32(cur_map_true)\n",
        "            cur_map_pred = np.float32(cur_map_pred)\n",
        "            \n",
        "            if scale_map:\n",
        "                cur_map_pred /=  np.abs(cur_map_pred).max() + 1e-16\n",
        "            \n",
        "            \n",
        "            cur_map_pred[cur_map_pred < threshold] = np.nan\n",
        "            cur_map_true[cur_map_true < threshold] = np.nan\n",
        "\n",
        "            axs[i].imshow(cur_image, cmap=image_cmap, aspect='equal')\n",
        "\n",
        "            if not np.all(np.isnan(cur_map_pred)):\n",
        "                axs[i].imshow(cur_map_pred, alpha=alpha, cmap=map_pred_cmap, aspect='equal')\n",
        "\n",
        "            if not np.all(np.isnan(cur_map_true)):\n",
        "                #pass\n",
        "                axs[i].imshow(cur_map_true, alpha=alpha, cmap=map_true_cmap, aspect='equal')\n",
        "\n",
        "            axs[i].set_title(title_format.format(titles[j_img]))\n",
        "            if axis_off:\n",
        "                axs[i].set_axis_off()\n",
        "            j_zx += 1\n",
        "        [axs[i].set_axis_off() for i in range(n_cols) if i >=_image.shape[0]]\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Jv_NZGgOVFm",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title model callback\n",
        "import keras\n",
        "from IPython.display import clear_output\n",
        "import pylab as plt\n",
        "\n",
        "from itertools import product\n",
        "\n",
        "\n",
        "class PlotLosses(keras.callbacks.Callback):\n",
        "  \n",
        "  \n",
        "    def __init__(self):\n",
        "      self.epoch_losses = {'ssim':[], 'precision':[], 'recall':[], 'fbeta_score':[],'iou':[]}\n",
        "      self.m_names = ['ssim', 'precision', 'recall', 'fbeta_score','iou']\n",
        "      self.e = 0\n",
        "      #0a0b0c\n",
        "      \n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.i = 0\n",
        "        self.x = []\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        self.fig = plt.figure()\n",
        "\n",
        "        self.logs = []\n",
        "        \n",
        "    def on_train_end(self, logs={}):\n",
        "        fig, ax = plt.subplots()\n",
        "        x = np.arange(1, self.e + 1)\n",
        "        plt.title(\"epoch: {}\".format(self.e + 1))\n",
        "        \n",
        "        for m in self.m_names:\n",
        "            ax.plot(x, self.epoch_losses[m], label=m)\n",
        "        axes = plt.gca()\n",
        "        axes.set_ylim([0,1])\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "    \n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        #if not logs:\n",
        "          #return\n",
        "        self.e = epoch\n",
        "        self.logs.append(logs)\n",
        "        self.x.append(self.i)\n",
        "        \n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.val_losses.append(logs.get('val_loss'))\n",
        "        self.i += 1\n",
        "        if self.i < self.params['epochs'] :\n",
        "            clear_output(wait=True)\n",
        "            l = round(float(logs.get('loss')), 4)\n",
        "            v_l = round(float(logs.get('val_loss')), 4)\n",
        "            #ssim = round(float(logs.get('ssim')), 4)\n",
        "            \n",
        "            m_values =[]\n",
        "            \n",
        "            for m in  self.m_names:\n",
        "                v = float(logs.get(m))\n",
        "                self.epoch_losses.get(m).append(v) \n",
        "                m_values.append(v)\n",
        "            \n",
        "            fig = plt.figure(figsize=(34, 20))\n",
        "            loss_plot = fig.add_subplot(4, 4, 1)\n",
        "            loss_plot.plot(self.x, self.losses, label=\"loss\")\n",
        "            loss_plot.plot(self.x, self.val_losses, label=\"v_loss\")\n",
        "            loss_plot.legend()\n",
        "            \n",
        "            p = fig.add_subplot(4,4,2)\n",
        "        \n",
        "            \n",
        "            x = np.arange(len(self.m_names))\n",
        "            p.bar(x, m_values)\n",
        "            axes = plt.gca()\n",
        "            axes.set_ylim([0,1])\n",
        "            plt.xticks(x, self.m_names)\n",
        "            \n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "            \n",
        "#l = PlotLosses()\n",
        "#l.on_epoch_end(1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gQnoDNkFMH5m",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title custom loss\n",
        "def weighted_categorical_crossentropy(weights):\n",
        "    \"\"\"\n",
        "    A weighted version of keras.objectives.categorical_crossentropy\n",
        "\n",
        "    Variables:\n",
        "        weights: numpy array of shape (C,) where C is the number of classes\n",
        "\n",
        "    Usage:\n",
        "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
        "        loss = weighted_categorical_crossentropy(weights)\n",
        "        model.compile(loss=loss,optimizer='adam')\n",
        "    \"\"\"\n",
        "\n",
        "    weights = keras.backend.variable(weights)\n",
        "\n",
        "    def loss(y_true, y_pred):\n",
        "        #print(y_true)\n",
        "        # scale predictions so that the class probas of each sample sum to 1\n",
        "        y_pred /= keras.backend.sum(y_pred, axis=-1, keepdims=True)\n",
        "        # clip to prevent NaN's and Inf's\n",
        "        y_pred = keras.backend.clip(y_pred, keras.backend.epsilon(), 1 - keras.backend.epsilon())\n",
        "        # calc\n",
        "        loss = y_true * keras.backend.log(y_pred) * weights\n",
        "        loss = -keras.backend.sum(loss, -1)\n",
        "        return loss\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWcwhcFc4wMf",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Metrics\n",
        "\n",
        "\n",
        "    \n",
        "def ssim(y_true, y_pred):\n",
        "    return tf.image.ssim(y_true, y_pred, 1.)\n",
        "  \n",
        "def iou(true, pred):\n",
        "\n",
        "    intersection = true * pred\n",
        "\n",
        "    notTrue = 1 - true\n",
        "    union = true + (notTrue * pred)\n",
        "\n",
        "    return K.sum(intersection)/K.sum(union)\n",
        "  \n",
        "def precision(y_true, y_pred):\n",
        "    \"\"\"Precision metric.\n",
        "    Only computes a batch-wise average of precision.\n",
        "    Computes the precision, a metric for multi-label classification of\n",
        "    how many selected items are relevant.\n",
        "    \"\"\"\n",
        "    y_true, y_pred = y_true[...,-1], y_pred[...,-1]\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    \"\"\"Recall metric.\n",
        "    Only computes a batch-wise average of recall.\n",
        "    Computes the recall, a metric for multi-label classification of\n",
        "    how many relevant items are selected.\n",
        "    \"\"\"\n",
        "    y_true, y_pred = y_true[...,-1], y_pred[...,-1]\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def fbeta_score(y_true, y_pred, beta=1):\n",
        "    \"\"\"Computes the F score.\n",
        "    The F score is the weighted harmonic mean of precision and recall.\n",
        "    Here it is only computed as a batch-wise average, not globally.\n",
        "    This is useful for multi-label classification, where input samples can be\n",
        "    classified as sets of labels. By only using accuracy (precision) a model\n",
        "    would achieve a perfect score by simply assigning every class to every\n",
        "    input. In order to avoid this, a metric should penalize incorrect class\n",
        "    assignments as well (recall). The F-beta score (ranged from 0.0 to 1.0)\n",
        "    computes this, as a weighted mean of the proportion of correct class\n",
        "    assignments vs. the proportion of incorrect class assignments.\n",
        "    With beta = 1, this is equivalent to a F-measure. With beta < 1, assigning\n",
        "    correct classes becomes more important, and with beta > 1 the metric is\n",
        "    instead weighted towards penalizing incorrect class assignments.\n",
        "    \"\"\"\n",
        "    if beta < 0:\n",
        "        raise ValueError('The lowest choosable beta is zero (only precision).')\n",
        "\n",
        "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
        "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
        "        return 0\n",
        "    y_true, y_pred = y_true[...,-1], y_pred[...,-1]\n",
        "    p = precision(y_true, y_pred)\n",
        "    r = recall(y_true, y_pred)\n",
        "    bb = beta ** 2\n",
        "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
        "    return fbeta_score\n",
        "  \n",
        "  \n",
        "METRICS = ['accuracy',\n",
        "           ssim, \n",
        "           precision, \n",
        "           recall, \n",
        "           fbeta_score,\n",
        "           iou,\n",
        "          ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_G3zcnG7AwW",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Load Lapteva_faults data\n",
        "import pickle\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "with open('/content/drive/My Drive/Seismic/Lapteva_faults_10k.pickle', 'rb') as handle:\n",
        "    data_set = pickle.load(handle)  \n",
        "\n",
        "X_set = data_set['X_set'] \n",
        "y_set = data_set['y_set'] \n",
        "Titles = data_set['fault_title'] \n",
        "#print(np.roll(y_set, -1, axis=2))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_set, y_set, test_size=.20, random_state=35\n",
        ")\n",
        "\n",
        "\n",
        "#Для unet прогноз подсвета \n",
        "#y_train = y_train[..., None]\n",
        "#y_test = y_test[..., None]\n",
        "\n",
        "# в каждом пикселе прогнозируем вероятность разлома и не разлома\n",
        "y_train = to_categorical(y_train, num_classes=2) \n",
        "y_test = to_categorical(y_test, num_classes=2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hrcuhq5WWMB9",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Unet model\n",
        "from keras.layers.merge import concatenate, add\n",
        "\n",
        "#@title Unet Sesm\n",
        "def conv2d_block(input_tensor, n_filters, kernel_size=3, batchnorm=True):\n",
        "    # first layer\n",
        "    x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=\"he_normal\",\n",
        "               padding=\"same\")(input_tensor)\n",
        "    if batchnorm:\n",
        "        x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    # second layer\n",
        "    x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=\"he_normal\",\n",
        "               padding=\"same\")(x)\n",
        "    if batchnorm:\n",
        "        x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    return x\n",
        "  \n",
        "def get_unet_arc(input_img, n_filters=16, dropout=0.5, batchnorm=True):\n",
        "    # contracting path\n",
        "    c1 = conv2d_block(input_img, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm)\n",
        "    p1 = MaxPooling2D((2, 2)) (c1)\n",
        "    p1 = Dropout(dropout*0.5)(p1)\n",
        "\n",
        "    c2 = conv2d_block(p1, n_filters=n_filters*2, kernel_size=3, batchnorm=batchnorm)\n",
        "    p2 = MaxPooling2D((2, 2)) (c2)\n",
        "    p2 = Dropout(dropout)(p2)\n",
        "\n",
        "    c3 = conv2d_block(p2, n_filters=n_filters*4, kernel_size=3, batchnorm=batchnorm)\n",
        "    p3 = MaxPooling2D((2, 2)) (c3)\n",
        "    p3 = Dropout(dropout)(p3)\n",
        "\n",
        "    c4 = conv2d_block(p3, n_filters=n_filters*8, kernel_size=3, batchnorm=batchnorm)\n",
        "    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
        "    p4 = Dropout(dropout)(p4)\n",
        "    \n",
        "    c5 = conv2d_block(p4, n_filters=n_filters*16, kernel_size=3, batchnorm=batchnorm)\n",
        "    \n",
        "    # expansive path\n",
        "    u6 = Conv2DTranspose(n_filters*8, (3, 3), strides=(2, 2), padding='same') (c5)\n",
        "    u6 = concatenate([u6, c4])\n",
        "    u6 = Dropout(dropout)(u6)\n",
        "    c6 = conv2d_block(u6, n_filters=n_filters*8, kernel_size=3, batchnorm=batchnorm)\n",
        "\n",
        "    u7 = Conv2DTranspose(n_filters*4, (3, 3), strides=(2, 2), padding='same') (c6)\n",
        "    u7 = concatenate([u7, c3])\n",
        "    u7 = Dropout(dropout)(u7)\n",
        "    c7 = conv2d_block(u7, n_filters=n_filters*4, kernel_size=3, batchnorm=batchnorm)\n",
        "\n",
        "    u8 = Conv2DTranspose(n_filters*2, (3, 3), strides=(2, 2), padding='same') (c7)\n",
        "    u8 = concatenate([u8, c2])\n",
        "    u8 = Dropout(dropout)(u8)\n",
        "    c8 = conv2d_block(u8, n_filters=n_filters*2, kernel_size=3, batchnorm=batchnorm)\n",
        "\n",
        "    u9 = Conv2DTranspose(n_filters*1, (3, 3), strides=(2, 2), padding='same') (c8)\n",
        "    u9 = concatenate([u9, c1], axis=3)\n",
        "    u9 = Dropout(dropout)(u9)\n",
        "    c9 = conv2d_block(u9, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm)\n",
        "    \n",
        "    #outputs = Conv2D(2, (1, 1), activation='softmax') (c9)\n",
        "    \n",
        "    h = Dense(8, activation='relu')(c9)\n",
        "\n",
        "    outputs = Dense(2, activation='softmax')(h)\n",
        "    \n",
        "    model = Model(inputs=[input_img], outputs=[outputs])\n",
        "    return model\n",
        "  \n",
        "  \n",
        "def create_unet():  \n",
        "    input_img = Input((64, 64, 1), name='img')\n",
        "    model = get_unet_arc(input_img, n_filters=16, dropout=0.05, batchnorm=True)\n",
        "    \n",
        "    \n",
        "    optimizer = keras.optimizers.Adam(\n",
        "        lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False\n",
        "    )\n",
        "    \n",
        "    loss = weighted_categorical_crossentropy(np.array([.1, 1]))\n",
        "    \n",
        "    \n",
        "    model.compile(optimizer='adam', loss=loss, metrics=METRICS)\n",
        "    #model.summary()\n",
        "    return model\n",
        "\n",
        "#create_unet_sesm()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzVCo3AOq3vn",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title my unet\n",
        "#@title Unet model\n",
        "from keras.layers.merge import concatenate, add\n",
        "\n",
        "#@title Unet Sesm\n",
        "def conv2d_block(input_tensor, n_filters, kernel_size=3, batchnorm=True):\n",
        "    # first layer\n",
        "    x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=\"he_normal\",\n",
        "               padding=\"same\")(input_tensor)\n",
        "    if batchnorm:\n",
        "        x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    # second layer\n",
        "    x = Conv2D(filters=n_filters, \n",
        "               kernel_size=(kernel_size, kernel_size), kernel_initializer=\"he_normal\",\n",
        "               padding=\"same\")(x)\n",
        "    if batchnorm:\n",
        "        x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    return x\n",
        "  \n",
        "def get_unet_arc(input_img, n_filters=16, dropout=0.5, batchnorm=True):\n",
        "    # contracting path\n",
        "    c1 = conv2d_block(input_img, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm)\n",
        "    p1 = MaxPooling2D((2, 2)) (c1)\n",
        "    p1 = Dropout(dropout*0.5)(p1)\n",
        "\n",
        "    c2 = conv2d_block(p1, n_filters=n_filters*2, kernel_size=3, batchnorm=batchnorm)\n",
        "    p2 = MaxPooling2D((2, 2)) (c2)\n",
        "    p2 = Dropout(dropout)(p2)\n",
        "\n",
        "    c3 = conv2d_block(p2, n_filters=n_filters*4, kernel_size=3, batchnorm=batchnorm)\n",
        "    p3 = MaxPooling2D((2, 2)) (c3)\n",
        "    p3 = Dropout(dropout)(p3)\n",
        "\n",
        "    c4 = conv2d_block(p3, n_filters=n_filters*8, kernel_size=3, batchnorm=batchnorm)\n",
        "    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
        "    p4 = Dropout(dropout)(p4)\n",
        "    \n",
        "    c5 = conv2d_block(p4, n_filters=n_filters*16, kernel_size=3, batchnorm=batchnorm)\n",
        "    \n",
        "    # expansive path\n",
        "    u6 = Conv2DTranspose(n_filters*8, (3, 3), strides=(2, 2), padding='same') (c5)\n",
        "    u6 = concatenate([u6, c4])\n",
        "    u6 = Dropout(dropout)(u6)\n",
        "    c6 = conv2d_block(u6, n_filters=n_filters*8, kernel_size=3, batchnorm=batchnorm)\n",
        "\n",
        "    u7 = Conv2DTranspose(n_filters*4, (3, 3), strides=(2, 2), padding='same') (c6)\n",
        "    u7 = concatenate([u7, c3])\n",
        "    u7 = Dropout(dropout)(u7)\n",
        "    c7 = conv2d_block(u7, n_filters=n_filters*4, kernel_size=3, batchnorm=batchnorm)\n",
        "\n",
        "    u8 = Conv2DTranspose(n_filters*2, (3, 3), strides=(2, 2), padding='same') (c7)\n",
        "    u8 = concatenate([u8, c2])\n",
        "    u8 = Dropout(dropout)(u8)\n",
        "    c8 = conv2d_block(u8, n_filters=n_filters*2, kernel_size=3, batchnorm=batchnorm)\n",
        "\n",
        "    u9 = Conv2DTranspose(n_filters*1, (3, 3), strides=(2, 2), padding='same') (c8)\n",
        "    u9 = concatenate([u9, c1], axis=3)\n",
        "    u9 = Dropout(dropout)(u9)\n",
        "    c9 = conv2d_block(u9, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm)\n",
        "    \n",
        "    #outputs = Conv2D(2, (1, 1), activation='softmax') (c9)\n",
        "    \n",
        "    h = Dense(8, activation='relu')(c9)\n",
        "\n",
        "    outputs = Dense(2, activation='softmax')(h)\n",
        "    \n",
        "    model = Model(inputs=[input_img], outputs=[outputs])\n",
        "    return model\n",
        "  \n",
        "  \n",
        "def create_my_unet():  \n",
        "    input_img = Input((64, 64, 1), name='img')\n",
        "    model = get_unet_arc(input_img, n_filters=16, dropout=0.05, batchnorm=True)\n",
        "    \n",
        "    \n",
        "    optimizer = keras.optimizers.Adam(\n",
        "        lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False\n",
        "    )\n",
        "    \n",
        "    loss = weighted_categorical_crossentropy(np.array([.1, 1]))\n",
        "    \n",
        "    \n",
        "    model.compile(optimizer='adam', loss=loss, metrics=METRICS)\n",
        "    #model.summary()\n",
        "    return model\n",
        "\n",
        "#create_unet_sesm()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W7uvyQYMaL7",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title fit\n",
        "\n",
        "\n",
        "model = create_unet()\n",
        "pltclb = PlotLosses()\n",
        "#print(model.summary())\n",
        "\n",
        "\n",
        "model.fit(\n",
        "    X_train[...,None],\n",
        "    y_train,\n",
        "    epochs=64,\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    callbacks=[pltclb],\n",
        "    validation_split= .1,\n",
        "    verbose=1,\n",
        "    validation_steps=None\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0VKuZxbULgv",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title evaluate\n",
        "l = model.evaluate(X_test[..., None], y_test, batch_size=64, verbose=0)\n",
        "\n",
        "for i,n in enumerate(model.metrics_names):\n",
        "    print(n + \": \" + str(l[i]))\n",
        "\n",
        "plt.subplot(111)\n",
        "x = np.arange(len(model.metrics_names))\n",
        "plt.bar(x, l)\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([0,1])\n",
        "plt.xticks(x, model.metrics_names)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpSvZ2Hcm6OD",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Rock (precision recall)\n",
        "\n",
        "# добавили 1 размерность в конце\n",
        "\n",
        "X = pred.reshape(pred.shape + (1,))\n",
        "y = y_test.reshape(y_test.shape + (1,))\n",
        "\n",
        "y_score = X[:, :, :, 1:]\n",
        "\n",
        "#фильтр по y где только разлом\n",
        "y_label = y[:, :, :, 1:] > 0.5\n",
        "\n",
        "y_score = y_score.reshape([y_score.size,])\n",
        "y_label = y_label.reshape([y_label.size,])\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_label, y_score)\n",
        "\n",
        "step_kwargs = ({'step': 'post'})\n",
        "               #if 'step' in signature(plt.fill_between).parameters\n",
        "               #else {})\n",
        "\n",
        "plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
        "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlim([0.0, 1.0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vebd7z3Ugzk9",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title show images\n",
        "\n",
        "pred = model.predict(X_test[..., None])\n",
        "print(pred.shape)\n",
        "sl_s = 120\n",
        "sl_e = 150\n",
        "show_image_dashboard(X_test[sl_s:sl_e], y_test[...,1][sl_s:sl_e],\n",
        "     pred[...,1][sl_s:sl_e], titles=Titles, fig_hight=4, threshold=.5, alpha=None, n_cols=6)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}